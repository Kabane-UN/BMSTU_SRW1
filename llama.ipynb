{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama, llama_print_system_info\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f058ca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>621</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>622</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>557</td>\n",
       "      <td>188393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>557</td>\n",
       "      <td>188394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>558</td>\n",
       "      <td>188507</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>558</td>\n",
       "      <td>188508</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>558</td>\n",
       "      <td>188509</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4998 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query-id  corpus-id  score\n",
       "0            0        620      2\n",
       "1            0        621      2\n",
       "2            0        622      2\n",
       "3            0        616      1\n",
       "4            0        617      1\n",
       "...        ...        ...    ...\n",
       "4993       557     188393      1\n",
       "4994       557     188394      1\n",
       "4995       558     188507      2\n",
       "4996       558     188508      2\n",
       "4997       558     188509      2\n",
       "\n",
       "[4998 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_json(\"corpus.jsonl\", lines=True)\n",
    "queries = pd.read_json(\"queries.jsonl\", lines=True)\n",
    "corpus.set_index(\"_id\", inplace=True)\n",
    "queries.set_index(\"_id\", inplace=True)\n",
    "qrels = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7696c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaJudge:\n",
    "  def __init__(self, model=\"models/google_gemma-3-4b-it-Q4_K_M.gguf\"):\n",
    "    self.model = model\n",
    "    self.llm = Llama(model,\n",
    "      n_gpu_layers=-1,\n",
    "      n_batch=32768, #16384\n",
    "      n_ctx=32768, #32768\n",
    "      flash_attn=True,\n",
    "      rope_scaling={\"type\": \"xpos\"},\n",
    "      verbose=False,\n",
    "    )\n",
    "  def generate_judgement(self, query, passage):\n",
    "    system = \"\"\"Ты — строгий эксперт по оценке качества поиска. Твоя задача — проанализировать Запрос и Отрывок текста, а затем оценить, насколько полезен этот Отрывок для ответа на Запрос.\n",
    "\n",
    "Используй строго следующую шкалу оценки (целое число от 0 до 2):\n",
    "\n",
    "[0] Нерелевантный: Отрывок не имеет отношения к запросу. Информация не помогает ответить на вопрос и не связана с темой даже косвенно.\n",
    "[1] Частично релевантный: Отрывок тематически связан с запросом, но не содержит прямого или полного ответа. В нем может обсуждаться контекст, но ключевой информации не хватает.\n",
    "[2] Полностью релевантный: Отрывок содержит прямой, точный и полный ответ на запрос. Пользователю не нужно искать дальше.\n",
    "\n",
    "Инструкции по формату:\n",
    "1. Твой ответ должен быть СТРОГО в формате JSON.\n",
    "2. Используй ключ \"score\" и целое число (0, 1 или 2).\n",
    "3. ЗАПРЕЩЕНО писать пояснения, рассуждения или любой другой текст вне JSON объекта.\n",
    "\n",
    "Пример 1:\n",
    "Запрос: \"Столица Франции\"\n",
    "Отрывок: \"Париж — столица и крупнейший город Франции, расположенный на севере страны.\"\n",
    "Ответ: {\"score\": 2}\n",
    "\n",
    "Пример 2:\n",
    "Запрос: \"Как приготовить борщ\"\n",
    "Отрывок: \"Борщ — это разновидность супа, популярная в Восточной Европе. Существует много споров о его происхождении.\"\n",
    "Ответ: {\"score\": 1}\n",
    "\n",
    "Пример 3:\n",
    "Запрос: \"Кто написал 'Войну и мир'?\"\n",
    "Отрывок: \"Рецепт яблочного пирога требует 1 кг яблок и 200 грамм сахара.\"\n",
    "Ответ: {\"score\": 0}\"\"\"\n",
    "    prompt = f\"\"\"Оцени релевантность следующей пары.\n",
    "\n",
    "Запрос: {query}\n",
    "Отрывок: {passage}\n",
    "\n",
    "Выведи только JSON с оценкой:\n",
    "\"\"\"\n",
    "    response = self.llm.create_chat_completion(\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": system},\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "      ], max_tokens=10, top_k=0, top_p=1, temperature=0,\n",
    "      response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"score\": {\"type\": \"integer\"}},\n",
    "            \"required\": [\"score\"],\n",
    "        },\n",
    "    },\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "#     response: ChatResponse = await self.client.chat(model=self.model, messages=[\n",
    "#       # {\n",
    "#       #   'role': 'system',\n",
    "#       #   'content': system_prompt\n",
    "#       # },\n",
    "#       {\n",
    "#         'role': 'user',\n",
    "#         'content': prompt,\n",
    "#       },\n",
    "# ], format={\n",
    "#   \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#       \"score\": {\n",
    "#         \"type\": \"integer\"\n",
    "#       },\n",
    "#     },\n",
    "#     \"required\": [\n",
    "#       \"score\",\n",
    "#     ]}, options={\n",
    "#       \"num_predict\": 32\n",
    "#     }, think=False)\n",
    "#     return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0d914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (32768) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "judge = LlamaJudge(\"models/google_gemma-3-4b-it-Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d819088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4998/4998 [1:38:28<00:00,  1.18s/it]  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "test = qrels.head(50)\n",
    "for idx, row in tqdm(qrels.iterrows(), total=len(qrels)):\n",
    "  true_score = row[\"score\"]\n",
    "  query = queries.loc[row[\"query-id\"]][\"text\"]\n",
    "  response = corpus.loc[row[\"corpus-id\"]][\"text\"]\n",
    "  res = judge.generate_judgement(query, response)\n",
    "  res = json.loads(res)\n",
    "  result = {\n",
    "    \"llm-score\": res[\"score\"],\n",
    "    \"human-score\": true_score,\n",
    "    \"query-id\": row[\"query-id\"],\n",
    "    \"corpus-id\": row[\"corpus-id\"]\n",
    "\n",
    "  }\n",
    "  results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9e3910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03355374393196309"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "llm = results_df[\"llm-score\"].to_numpy(int)\n",
    "human = results_df[\"human-score\"].to_numpy(int)\n",
    "cohen_kappa_score(llm, human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b703461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG:  0.8597346124614438\n",
      "NRMSE:  0.41438953591396344\n",
      "NMAE:  0.34103641456582634\n",
      "kendalltau:  0.5098321958450986\n",
      "RBO:  0.7566482599317313\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import metrics\n",
    "\n",
    "\n",
    "print(\"NDCG: \", metrics.calc_ndcg(results_df))\n",
    "print(\"NRMSE: \", metrics.calc_nrmse(results_df))\n",
    "print(\"NMAE: \", metrics.calc_nmae(results_df))\n",
    "print(\"kendalltau: \", metrics.calc_kendalltau(results_df))\n",
    "print(\"RBO: \", metrics.calc_rbo(results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bcaf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_df).to_csv('gemma3:4b_gemini.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a68328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0],\n",
       "       [ 783, 1330, 2546],\n",
       "       [  12,   56,  271]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "llm = results_df[\"llm-score\"].to_numpy(int)\n",
    "human = results_df[\"human-score\"].to_numpy(int)\n",
    "confusion_matrix(human, llm, labels=range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8be1d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = results_df[(results_df['llm-score'] == 2) & (results_df['human-score'] == 1)]\n",
    "# for _, row in res.iterrows():\n",
    "#   if row[\"query-id\"] == 0:\n",
    "#     continue\n",
    "#   print(\"Query: \", queries.loc[row[\"query-id\"]][\"text\"])\n",
    "#   print(\"Passage: \", corpus.loc[row[\"corpus-id\"]][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
